{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7301cb2",
   "metadata": {},
   "source": [
    "## 4 Implementing a GPT Model from scratch to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe85467",
   "metadata": {},
   "source": [
    "### 4.1 Coding an LLM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3de479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7bd7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Placeholder for Transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Layernorm placeholder\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # just a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # placholder\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a366c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1))) # encode the text into a tensor\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0) # stacking our text\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7d48908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(cfg = GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdde7ce8",
   "metadata": {},
   "source": [
    "### 4.2 Normalizing Activations with Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca36b02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
       "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_example = torch.randn(2, 5)\n",
    "batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bff780d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
       "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer= nn.Sequential(\n",
    "    nn.Linear(5, 6),\n",
    "    nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5bdcd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1324],\n",
       "        [0.2170]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1eed11ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0231],\n",
       "        [0.0398]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = out.var(dim=-1, keepdim=True)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee6451c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = ((out - mean)/torch.sqrt(var))\n",
    "norm.var(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53fd7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        # initialized at one, but can learn to scale differently\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # initialized at zero, but can learn to shift differently\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfb61fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6745,  1.5470, -0.9549,  0.6431, -0.9549, -0.9549],\n",
       "        [-0.0207,  0.1228, -1.1913,  1.6619,  0.6186, -1.1913]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = LayerNorm(6)\n",
    "outputs_normed = ln(out)\n",
    "outputs_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce7d42d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00],\n",
       "        [3.9736e-08]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_normed.mean(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8b1e780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1994],\n",
       "        [1.1996]], grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_normed.var(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c911b5",
   "metadata": {},
   "source": [
    "### 4.3 Implementing a feed forward network with GELU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23aa632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32710411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"])\n",
    "        self.gelu = GELU()\n",
    "        self.fc2 = nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.gelu,\n",
    "            self.fc2,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08dae3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c5b39cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 768)\n",
    "ffn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4f0935f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0223,  0.0308,  0.0022,  ..., -0.0174, -0.0227, -0.0178],\n",
       "        [-0.0027, -0.0043, -0.0274,  ...,  0.0079,  0.0112,  0.0010],\n",
       "        [-0.0029,  0.0175,  0.0244,  ..., -0.0164, -0.0300,  0.0045],\n",
       "        ...,\n",
       "        [ 0.0255,  0.0023,  0.0277,  ..., -0.0037, -0.0134, -0.0284],\n",
       "        [ 0.0266,  0.0236, -0.0193,  ...,  0.0120,  0.0061, -0.0261],\n",
       "        [ 0.0222,  0.0106, -0.0190,  ..., -0.0253,  0.0320, -0.0109]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80fb17",
   "metadata": {},
   "source": [
    "### 4.4 Adding Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6abcadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet paper\n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                # don't add a shortcut connection to the output\n",
    "                x = x +layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "    \n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b201396",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e34cca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173590746708214\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152042235247791\n",
      "layers.3.0.weight has gradient mean of 0.0013988739810883999\n",
      "layers.4.0.weight has gradient mean of 0.00504964729771018\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e39dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694102346897125\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f560d",
   "metadata": {},
   "source": [
    "### 4.5 Connecting attention and linear layers in a transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44ab7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # We have two LayerNorm objects because they have trainable parameters that are being used in TWO DIFFERENT places, where TWO DIFFERENT sets of params must be learnt\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x # store x in shortcut for residual stream addition later on\n",
    "        x = self.norm1(x) # normalize input\n",
    "        x = self.att(x) \n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # adding back the residual\n",
    "\n",
    "        # Shortcut connection for feedforward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4e8a08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "output = block(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f593844d",
   "metadata": {},
   "source": [
    "### 4.6 Coding the GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4379626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b06451",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(cfg = GPT_CONFIG_124M)\n",
    "out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44d9af5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1af0c62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b2d7667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163009536"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94540417",
   "metadata": {},
   "source": [
    "### 4.7 Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e6770",
   "metadata": {},
   "source": [
    "In other words, converting the logit output of the GPT model into text predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ded9228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] # Truncate to our context size (don't want model to crash)\n",
    "        \n",
    "        with torch.no_grad(): # suppressing the generation of the gradient computation graph because we are not training the model \n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :] # get the last row of the outputted logits (predicting next word)\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5fbba9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:   [15496, 11, 314, 716]\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:  \", encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "433e6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_tensor.shape:  torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape: \", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bad996e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e317ea74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,    11,   314,   716,  5466, 45044, 28081,  2204, 24924, 24741]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5e534234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am sale caregivers Clause benef bloilibrium'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec84618",
   "metadata": {},
   "source": [
    "The above is gibberish because we have not actually trained the model weights!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
